{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import seaborn as sns\n",
    "import heapq # for retrieval topK\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pprint import pprint \n",
    "from time import time\n",
    "from scipy import spatial\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in log data and clean it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative_test_rating_19N.txt',\n",
      " 'etu_log.csv',\n",
      " 'Content-based.ipynb',\n",
      " 'Negative_test_rating_199N.txt',\n",
      " 'README.md',\n",
      " 'doc2_conversion_table.csv',\n",
      " 'cw-article.csv',\n",
      " 'doc1_viewing_columnExp.csv',\n",
      " 'doc1_viewing_history_cateID.csv',\n",
      " 'article_contents.csv',\n",
      " 'doc3_post_and_clicks.csv',\n",
      " 'DailyPost.csv',\n",
      " 'Negative_test_rating_50N.txt',\n",
      " 'EDA.ipynb',\n",
      " 'doc2_conversion_reference.csv',\n",
      " 'CF.ipynb',\n",
      " 'Negative_test_rating_99N.txt',\n",
      " 'doc1_viewing_data.csv']\n",
      "['output.json',\n",
      " 'optimization_test.py',\n",
      " 'multilingual.md',\n",
      " 'LICENSE',\n",
      " 'bert_train_data.ipynb',\n",
      " 'input.txt',\n",
      " 'tokenization.py',\n",
      " 'create_pretraining_data.py',\n",
      " 'README.md',\n",
      " 'optimization.py',\n",
      " 'run_pretraining.py',\n",
      " 'extract_features.py',\n",
      " 'requirements.txt',\n",
      " 'tokenization_test.py',\n",
      " 'chinese_L-12_H-768_A-12.zip',\n",
      " 'run_classifier.py',\n",
      " 'run_squad.py',\n",
      " 'sample_text.txt',\n",
      " 'bert_dev.p',\n",
      " '__init__.py',\n",
      " 'run_classifier_with_tfhub.py',\n",
      " 'CONTRIBUTING.md',\n",
      " 'bert_train.p',\n",
      " 'modeling.py',\n",
      " 'modeling_test.py',\n",
      " '.gitignore']\n",
      "['vocab.py', 'lm_dataset.py', 'dataset.py', '__init__.py']\n",
      "['vocab.cpython-36.pyc', '__init__.cpython-36.pyc']\n",
      "['bert_model.ckpt.index',\n",
      " 'bert_config.json',\n",
      " 'bert_model.ckpt.meta',\n",
      " 'bert_model.ckpt.data-00000-of-00001',\n",
      " 'vocab.txt']\n",
      "['vocab-checkpoint.txt', 'bert_config-checkpoint.json']\n",
      "['optimization.cpython-36.pyc',\n",
      " 'modeling.cpython-36.pyc',\n",
      " 'tokenization.cpython-36.pyc']\n",
      "['index', 'HEAD', 'packed-refs', 'description', 'config']\n",
      "[]\n",
      "[]\n",
      "['master']\n",
      "[]\n",
      "['HEAD']\n",
      "['commit-msg.sample',\n",
      " 'post-update.sample',\n",
      " 'applypatch-msg.sample',\n",
      " 'prepare-commit-msg.sample',\n",
      " 'update.sample',\n",
      " 'pre-applypatch.sample',\n",
      " 'pre-commit.sample',\n",
      " 'pre-rebase.sample',\n",
      " 'pre-push.sample']\n",
      "[]\n",
      "[]\n",
      "['pack-b6d12db212fde707abb21120c4a58265c745da8f.pack',\n",
      " 'pack-b6d12db212fde707abb21120c4a58265c745da8f.idx']\n",
      "[]\n",
      "['exclude']\n",
      "['HEAD']\n",
      "[]\n",
      "['master']\n",
      "[]\n",
      "['HEAD']\n",
      "['checkpoint',\n",
      " 'graph.pbtxt',\n",
      " 'train.tf_record',\n",
      " 'model.ckpt-114100.data-00000-of-00001',\n",
      " 'eval.tf_record',\n",
      " 'model.ckpt-114100.meta',\n",
      " 'model.ckpt-114100.index',\n",
      " 'events.out.tfevents.1561712249.jupyter-yaoting-40aiacademy-2etw',\n",
      " 'eval_results.txt']\n",
      "['events.out.tfevents.1561750545.jupyter-yaoting-40aiacademy-2etw']\n",
      "[]\n",
      "['create_pretraining_data-checkpoint.py',\n",
      " 'run_classifier-checkpoint.py',\n",
      " 'CONTRIBUTING-checkpoint.md',\n",
      " 'run_squad-checkpoint.py',\n",
      " 'bert_train_data-checkpoint.ipynb']\n",
      "['CF-checkpoint.ipynb',\n",
      " 'Content-based-checkpoint.ipynb',\n",
      " 'EDA-checkpoint.ipynb']\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 4359075: unexpected end of data\n"
     ]
    }
   ],
   "source": [
    "for root,dirs,files in os.walk('.'):\n",
    "    pprint(files)\n",
    "    file_list = files\n",
    "df_etu_log = pd.read_csv('etu_log.csv', engine = 'python',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing\n",
      "Done_1,df_shape is : (2154549, 16)\n",
      "Done_2,df_shape is : (1993796, 16)\n",
      "Done_3,df_shape is : (1993796, 16)\n",
      "Done_4,df_shape is : (1993796, 17)\n"
     ]
    }
   ],
   "source": [
    "def del_df_columns_list(df,column_name,name_list,way_is_1=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    it can only delete one columns once \n",
    "    \"\"\"\n",
    "    if way_is_1 == True:\n",
    "        for i in name_list:\n",
    "            df = df[df[column_name] != i]\n",
    "    else:\n",
    "    #     below is the pd.merge way to merge data, but sometime coz pd will mislead the columns due to dtypes\n",
    "    #     If using the merge way, we should get the list that we want to PERSERVE.    \n",
    "        d = pd.DataFrame(data = name_list, columns = [column_name])\n",
    "        df = pd.merge(df, d, how = 'inner', on = [column_name])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('start preprocessing')\n",
    "### Delete the unique user with click time less than 2, for 1 clicking user for CF-model is useless\n",
    "del_list = df_etu_log.eruid.value_counts()[(df_etu_log.eruid.value_counts() > 2) & (df_etu_log.eruid.value_counts() < 90*50)].keys()\n",
    "df_etu_log = del_df_columns_list(df_etu_log,'eruid',del_list,way_is_1 = False)\n",
    "print('Done_1,df_shape is :',df_etu_log.shape)\n",
    "### Drop the na-value in pid,eruid\n",
    "df_etu_log = df_etu_log.dropna(subset = ['pid','eruid'],how = 'any')\n",
    "print('Done_2,df_shape is :',df_etu_log.shape)\n",
    "### The time format should be like '**-**-**', so we filter out those length not equal to 8.\n",
    "del_list = list(set([i for i in df_etu_log.time if len(i)!=8])) \n",
    "df_etu_log = del_df_columns_list(df_etu_log,'time',del_list)\n",
    "print('Done_3,df_shape is :',df_etu_log.shape)\n",
    "### Creating the column using both time and date\n",
    "df_etu_log['datetime'] = df_etu_log['dates'] +'-'+ df_etu_log['time']\n",
    "print('Done_4,df_shape is :',df_etu_log.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in content related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45686, 8)\n"
     ]
    }
   ],
   "source": [
    "### clean content info\n",
    "df_content = pd.read_csv('article_contents.csv').drop(columns = ['Unnamed: 0'])\n",
    "df_cate = pd.read_csv('cw-article.csv').rename({'id':'pid'},axis = 'columns')\n",
    "### about 300 articles have mutiple class, not a huge number, but if we want to improve the performance of classifer, it might be better to use a ML to re-classify the labels\n",
    "### we keep the first class only here\n",
    "df_cate = pd.DataFrame.drop_duplicates(df_cate , subset = 'pid',keep = 'first')\n",
    "\n",
    "### merge the content info\n",
    "df_content = pd.merge(df_content,df_cate[['pid','author','type','masterChannelTitle','onlineTime']],how = 'left',on = 'pid')\n",
    "### To fine-tune the bert, all data must have label, so around 200 article are without labels, drop them first.\n",
    "df_content = df_content.dropna(axis = 0,subset = ['masterChannelTitle'] )\n",
    "\n",
    "### cleaning the sentences and remove pattern like <p>, </p>, etc..\n",
    "df_content['文章內容'] = df_content['文章內容'].str.replace('<.*?>','')\n",
    "df_content['文章內容'] = df_content['文章內容'].str.replace('\\r|&[a-z]','')\n",
    "df_content['文章內容'] = df_content['文章內容'].replace('',np.nan)\n",
    "list_content_empty = []\n",
    "pid_content_empty = []\n",
    "for i in range(len(df_content)):\n",
    "    if type(df_content.iloc[i,2]) is not str:\n",
    "        pid_content_empty.append(df_content.iloc[i,0])\n",
    "        list_content_empty.append(df_content.index[i])\n",
    "df_content.drop(list_content_empty, inplace = True)\n",
    "\n",
    "### label-encode the master channel to make it compatible to bert fine-tune config.\n",
    "cate_dict = {\n",
    "    '教育':0,\n",
    "    '產業':1, \n",
    "    '財經時事':2,\n",
    "    '經營管理':3, \n",
    "    '政治社會':4, \n",
    "    '經濟學人':5, \n",
    "    '環境':6, \n",
    "    '人物觀點':7,\n",
    "    '專欄':8,\n",
    "    '國際':9, \n",
    "    '健康關係':10, \n",
    "    '時尚生活':11, \n",
    "    '調查':12, \n",
    "    '數據圖表':13,\n",
    "    '天下雜誌部落格':14, \n",
    "    '互動圖表':15, \n",
    "    '@想像未來':16,\n",
    "    '天下Talk':17}\n",
    "df_content['cate_label'] = df_content.apply(lambda w:cate_dict[w.masterChannelTitle],axis = 1)\n",
    "\n",
    "print(df_content.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load in bert vector\n",
    "bert_vector = []\n",
    "with open('bert_vector/output.json') as file:\n",
    "    for line in file.readlines():\n",
    "        bert_vector.append(json.loads(line))\n",
    "        \n",
    "df_content = df_content.reset_index().join(pd.DataFrame(bert_vector).features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create pid-vector dictionary\n",
    "pid_vector = {}\n",
    "for i in range(len(df_content)):\n",
    "    pid_vector[df_content['pid'].values[i]] = df_content['features'].values[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hit rate / NDCG\n",
    "### reference : https://www.comp.nus.edu.sg/~xiangnan/papers/cikm15-trirank-cr.pdf\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    if gtItem in ranklist:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    ar = np.array(ranklist)\n",
    "    if gtItem in ar:\n",
    "        return math.log(2) / math.log(np.where(ar == gtItem)[0][0] + 2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_rating(idx, eval_mode, uim, pid_vector,test_user_last_pid):\n",
    "    \n",
    "    '''''\n",
    "    eval_mode = 'Keras', 'ALS', 'matrix'\n",
    "    '''''\n",
    "    rating = _testRatings[idx]\n",
    "    ### rating should be like (  [user_id,article_pid(only one article, which is ground truth)]  ) \n",
    "    items = _testNegatives[idx][1]\n",
    "    ### items should be like (  [article_pid(19 ones, which are ones the reader haven't read)]  )\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    items.append(gtItem)\n",
    "    \n",
    "    # Get prediction scores, the process is we offer 100(99 negative + 1 ground truth) articles to one user in testing data, and predict the score(read or not) and ranking.\n",
    "    map_item_score = {}\n",
    "#     users = np.full(len(items), u, dtype = 'int32')\n",
    "\n",
    "    if eval_mode == 'ALS':\n",
    "        predictions = _model.rank_items(u, uim.T, items)\n",
    "        items.pop()\n",
    "        ranklist = np.array(predictions, dtype = int)[:_K,0]\n",
    "        \n",
    "    else:\n",
    "        if eval_mode == 'Keras':\n",
    "            predictions = _model.predict([users, np.array(items)], \n",
    "                                         batch_size=100, verbose=0)\n",
    "        elif eval_mode == 'matrix':         \n",
    "            predictions = _model[u,items]\n",
    "        \n",
    "        elif eval_mode == 'content-based':\n",
    "            predictions = []\n",
    "            pid_vectors = []\n",
    "            user_history_article_list = test_user_last_pid[u]\n",
    "            for pid in user_history_article_list:\n",
    "                if pid in pid_vector:\n",
    "                    pid_vectors.append(pid_vector[pid])\n",
    "            user_history_article_mean = np.array(pid_vectors).mean(axis = 0)\n",
    "        \n",
    "            for i in range(len(items)):\n",
    "                if items[i] in pid_vector:\n",
    "                    similarity = 1 - spatial.distance.cosine(user_history_article_mean,pid_vector[items[i]])\n",
    "                    predictions.append(similarity)\n",
    "                else:\n",
    "                    predictions.append(0)\n",
    "        for i in range(len(items)):\n",
    "            item = items[i]\n",
    "            map_item_score[item] = predictions[i]\n",
    "        items.pop()\n",
    "        \n",
    "        # Evaluate top rank list\n",
    "        ranklist = heapq.nlargest(_K, map_item_score, key=map_item_score.get)\n",
    "        \n",
    "    hr = getHitRatio(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    return (hr, ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables that are shared across processes\n",
    "_model = None\n",
    "_testRatings = None\n",
    "_testNegatives = None\n",
    "_K = None\n",
    "\n",
    "def evaluate_model(model, testRatings, testNegatives, K, num_thread, eval_mode = 'Keras', uim = None,\n",
    "                   pid_vector = None, test_user_last_pid = None):\n",
    "    \"\"\"\n",
    "    Evaluate the performance (Hit_Ratio, NDCG) of top-K recommendation\n",
    "    Return: score of each test rating.\n",
    "    \"\"\"\n",
    "    global _model\n",
    "    global _testRatings\n",
    "    global _testNegatives\n",
    "    global _K\n",
    "    _model = model\n",
    "    _testRatings = testRatings\n",
    "    _testNegatives = testNegatives\n",
    "    _K = K\n",
    "        \n",
    "    hits, ndcgs = [],[]\n",
    "    if(num_thread > 1): # Multi-thread\n",
    "        pool = multiprocessing.Pool(processes=num_thread)\n",
    "        res = pool.map(eval_one_rating, range(len(_testRatings)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        hits = [r[0] for r in res]\n",
    "        ndcgs = [r[1] for r in res]\n",
    "        return (hits, ndcgs)\n",
    "    else:# Single thread\n",
    "        for idx in range(len(_testRatings)):\n",
    "            (hr,ndcg) = eval_one_rating(idx, eval_mode, uim, pid_vector, test_user_last_pid)\n",
    "            hits.append(hr)\n",
    "            ndcgs.append(ndcg)      \n",
    "    return (hits, ndcgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Testing Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this section, I hope that I don't just randomly split the training/testing sets into 9:1.\n",
    "#### Rather, I will select those users with over 5 click times and pick the last click as the testing set.\n",
    "#### For example, index 1354 user has following reading history:{2,364,8796,4511,5,64}.\n",
    "#### This user history for {2,364,8796,4511,5} will be viewed as training set and 64 will be testing.\n",
    "#### Here testing means 1354 user did click into the article 64, and If the recommendation system provides the article in the recommending list, it means it succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_time(df , columns_1 , columns_2 , columns_time , ratio = 0.9):\n",
    "    '''\n",
    "    we will use the columns_1 as the first level index\n",
    "    sort the columns_2 as the target by columns_time\n",
    "    '''\n",
    "    \n",
    "    retreive_name = df[columns_1].value_counts()[df[columns_1].value_counts()>5].keys()\n",
    "    df_test = df[df[columns_1].isin(retreive_name)]\n",
    "    df_test = df_test.sort_values(columns_time).groupby(columns_1).tail(1)\n",
    "    df_train = df.drop(index=df_test.index)\n",
    "    \n",
    "    if df_test.shape[0] + df_train.shape[0] == df.shape[0]:\n",
    "        print('train_test_split succeed!! with df_train shape:(%d,%d), df_test shape:(%d,%d)'\n",
    "              %(df_train.shape[0],df_train.shape[1],df_test.shape[0],df_test.shape[1])\n",
    "             )\n",
    "        return df_train, df_test, retreive_name\n",
    "    else:\n",
    "        print('Oops, something wrong, with df_train shape : (%d,%d), df_test shape : (%d,%d)'\n",
    "              %(df_train.shape[0],df_train.shape[1],df_test.shape[0],df_test.shape[1])\n",
    "             )\n",
    "        print('df_shape : (%d,%d)'\n",
    "              %(df.shape[0],df.shape[1])\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_test_split succeed!! with df_train shape:(1894191,17), df_test shape:(99605,17)\n"
     ]
    }
   ],
   "source": [
    "df_train,df_test, retrieve = train_test_split_time(df_etu_log,'eruid','pid','datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the leave-one-out method to evaluate our testing data\n",
    "#### Basically, we will generate N samples list that the test user hasn't read and add the ground truth into the list, and see how the recommendation system will score and rank the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampling(df_target,df_source,numbers_of_N_sample, generate_negative = False):\n",
    "    '''\n",
    "    1.\n",
    "    return: test_rating in the shape of [ [user_pid,article] *99605 ]\n",
    "    2.\n",
    "    from the target dataframe, sampling the negative sample from source dataframe\n",
    "    return: list in the shape of [ [(2(user_id,ground truth)) , [19(negative samples)] ]*99605]\n",
    "    '''\n",
    "    \n",
    "    test_rating = df_target[['eruid','pid']].values.tolist()\n",
    "    \n",
    "    print('test_rating yield successfully!!!')\n",
    "    \n",
    "    if generate_negative == True:\n",
    "        df_temp = df_source.drop_duplicates(subset = 'pid', keep = 'first')\n",
    "        negative_test_rating = []\n",
    "        for i in range(df_target.shape[0]):\n",
    "            drop_id = test_rating[i][0]\n",
    "            list_ = df_temp[df_temp['eruid'] != drop_id]['pid'].sample(numbers_of_N_sample).values.tolist()\n",
    "            \n",
    "            negative_test_rating.append([test_rating[i],list_])\n",
    "        print('negative_test_rating yield successfully!!!')\n",
    "\n",
    "        return test_rating, negative_test_rating\n",
    "    else:\n",
    "        return test_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only for generating sample, if there is a sample file already, skip this cell in the future\n"
     ]
    }
   ],
   "source": [
    "print('''only for generating sample, if there is a sample file already, skip this cell in the future''')\n",
    "### In the future just execute the next cell to get the test rating & negative sampling\n",
    "\n",
    "# time1 = time()\n",
    "# test_rating, negative_test_rating = get_sampling(df_test,df_train,19,True)\n",
    "# time2 = time()\n",
    "# print('Took for %d seconds' %(time2-time1))\n",
    "# with open(\"Negative_test_rating_19N.txt\",\"wb\") as f: #in write mode\n",
    "#     pickle.dump(negative_test_rating,f)\n",
    "\n",
    "    \n",
    "# ### For the simple model already kill(98% Hit Ratio) the dataset, so let's make it a bit harder\n",
    "# time1 = time()\n",
    "# test_rating, negative_test_rating = get_sampling(df_test,df_train,99,True)\n",
    "# time2 = time()\n",
    "# print('Took for %d seconds' %(time2-time1))\n",
    "# with open(\"Negative_test_rating_99N.txt\",\"wb\") as f: #in write mode\n",
    "#     pickle.dump(negative_test_rating,f)\n",
    "\n",
    "    \n",
    "### Experiment on my theory: whether the N samples in hit ratio is a parameters that needs to tune\n",
    "# time1 = time()\n",
    "# test_rating, negative_test_rating = get_sampling(df_test,df_train,199,True)\n",
    "# time2 = time()\n",
    "# print('Took for %d seconds' %(time2-time1))\n",
    "# with open(\"Negative_test_rating_199N.txt\",\"wb\") as f: #in write mode\n",
    "#     pickle.dump(negative_test_rating,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rating yield successfully!!!\n",
      "negative_test_rating yield successfully!!!\n",
      "Took for 4 seconds\n"
     ]
    }
   ],
   "source": [
    "time1 = time()\n",
    "test_rating = get_sampling(df_test,df_train,None)\n",
    "with open(\"Negative_test_rating_199N.txt\",'rb') as f: #in read mode, not in write mode, careful\n",
    "    negative_test_rating = pickle.load(f)\n",
    "    print('negative_test_rating yield successfully!!!')\n",
    "time2 = time()\n",
    "print('Took for %d seconds' %(time2-time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create the dictionary test user last read in training data \n",
    "def test_user_last_pid_func(number_of_last_seen):\n",
    "    ### Create the last seen dict\n",
    "    test_user_last_pid = {}\n",
    "    retrieve_name = df_test.eruid.values\n",
    "    df_train_lookup = df_train[df_train['eruid'].isin(retrieve_name)].sort_values('datetime')\n",
    "    df_train_lookup = df_train_lookup[['eruid','pid','datetime']].drop_duplicates(subset = 'eruid', keep = 'last')\n",
    "    for i in range(len(df_train_lookup)):\n",
    "        test_user_last_pid[df_train_lookup['eruid'].values[i]]  = [df_train_lookup['pid'].values[i]]\n",
    "    ### Append the last nth seen dict\n",
    "    if number_of_last_seen > 1:\n",
    "        for number in range(2,number_of_last_seen+1):\n",
    "            df_train_lookup_append = df_train[df_train['eruid'].isin(retrieve_name)].sort_values('datetime')[['pid','eruid']].groupby(['eruid'],as_index = False).nth(-number)\n",
    "            for i in range(len(df_train_lookup_append)):\n",
    "                test_user_last_pid[df_train_lookup_append['eruid'].values[i]].append(df_train_lookup_append['pid'].values[i])\n",
    "    \n",
    "    return test_user_last_pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For conten-based recommendation, the recommending way is quite simple. We recommend the item that is the most similar to one the user has clicked before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this part, we will first make the matrix to be like 1 for read once 0 for never-read \n",
    "df_train['read_time'] = 1\n",
    "df_test['read_time'] = 1\n",
    "df_train_group = df_train[['eruid','pid','read_time']].groupby(by = ['eruid','pid'],as_index=False).sum()\n",
    "df_test_group = df_test[['eruid','pid','read_time']].groupby(by = ['eruid','pid'],as_index=False).sum()\n",
    "\n",
    "### For the reason that normally people don't read articles more than 10 times\n",
    "cliper = 10\n",
    "df_train_group['read_time'] = df_train_group['read_time'].apply(lambda w: min(cliper,w))\n",
    "\n",
    "\n",
    "### To feed into the keras model, we have to turn both the eruid and pid into integer index\n",
    "eruid_map = {i:v for i,v in enumerate(df_etu_log.eruid.unique())}\n",
    "inverse_eruid_map = {v:i for i,v in enumerate(df_etu_log.eruid.unique())}\n",
    "pid_map = {i:v for i,v in enumerate(df_etu_log.pid.unique())}\n",
    "inverse_pid_map = {v:i for i,v in enumerate(df_etu_log.pid.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating a mapping table for training data\n",
    "df_train_group_map = df_train_group.copy()\n",
    "df_test_group_map = df_test_group.copy()\n",
    "df_train_group_map['eruid'] = df_train_group['eruid'].map(inverse_eruid_map)\n",
    "df_train_group_map['pid'] = df_train_group['pid'].map(inverse_pid_map)\n",
    "df_test_group_map['eruid'] = df_test_group['eruid'].map(inverse_eruid_map)\n",
    "df_test_group_map['pid'] = df_test_group['pid'].map(inverse_pid_map)\n",
    "\n",
    "### creating a mapping list for testing data   \n",
    "test_rating_map = []\n",
    "for i in range(len(test_rating)):\n",
    "    test_rating_map.append([inverse_eruid_map[test_rating[i][0]],inverse_pid_map[test_rating[i][1]]])\n",
    "\n",
    "\n",
    "### creating a mapping list for negative testing data\n",
    "negative_test_rating_map = []\n",
    "for i in range(len(negative_test_rating)):\n",
    "    negative_test_rating_map.append([\n",
    "                                     [inverse_eruid_map[negative_test_rating[i][0][0]],inverse_pid_map[negative_test_rating[i][0][1]]],\n",
    "                                     list(map(lambda w:inverse_pid_map[w],negative_test_rating[i][1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_group_map shape:  (1555002, 3)\n",
      "number of users:  368598\n",
      "number of items:  19988\n",
      "The sparse matrix is one with shape (368598 , 19988), with 1555002 non-zero read_times\n"
     ]
    }
   ],
   "source": [
    "print('df_train_group_map shape: ',df_train_group_map.shape)\n",
    "print('number of users: ', len(eruid_map.items()))\n",
    "print('number of items: ', len(pid_map.items()))\n",
    "print('The sparse matrix is one with shape (%d , %d), with %d non-zero read_times'\n",
    "      %(len(eruid_map.items()),len(pid_map.items()),df_train_group_map.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:37: RuntimeWarning: Mean of empty slice.\n",
      "/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR = 0.2050, NDCG = 0.1786 [2438.0 s] number of last seen = 0\n",
      "HR = 0.2050, NDCG = 0.1786 [2439.3 s] number of last seen = 1\n",
      "HR = 0.1855, NDCG = 0.1441 [2446.7 s] number of last seen = 2\n",
      "HR = 0.1706, NDCG = 0.1298 [2447.7 s] number of last seen = 3\n",
      "HR = 0.1578, NDCG = 0.1182 [2455.3 s] number of last seen = 4\n"
     ]
    }
   ],
   "source": [
    "### Decide how many last pieces of articles to calculate\n",
    "number_of_seen_article = [i for i in range(5)]\n",
    "for number in number_of_seen_article:\n",
    "    test_user_last_pid = test_user_last_pid_func(number)\n",
    "\n",
    "    ### Evaluate how the 'most similar content recommendation works'\n",
    "    topK = 5\n",
    "    verbose = 0\n",
    "    evaluation_threads = 1\n",
    "\n",
    "    time_1 = time()\n",
    "    testRatings, testNegatives = test_rating, negative_test_rating\n",
    "    time1 = time()\n",
    "    (hits, ndcgs) = evaluate_model(None, testRatings, testNegatives, topK, evaluation_threads,\n",
    "                                   eval_mode = 'content-based', pid_vector = pid_vector,\n",
    "                                   test_user_last_pid = test_user_last_pid)\n",
    "    hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "\n",
    "    print('HR = %.4f, NDCG = %.4f [%.1f s] number of last seen = %d' % (hr, ndcg, time()-time1,number))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
